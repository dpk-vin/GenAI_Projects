{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ecc4ab4",
   "metadata": {},
   "source": [
    "\n",
    "# Email Search AI — Semantic Search, Vector DB & Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**Goal:** Build a generative search system for an organization's email corpus to *find and validate past decisions, strategies, and data* across large email threads.\n",
    "\n",
    "1. **Embedding Layer** — preprocessing, cleaning, chunking strategies, and embeddings (OpenAI or SentenceTransformers).\n",
    "2. **Search Layer** — vector database (Chroma), query generation, caching, and reranking (cross-encoders).\n",
    "3. **Generation Layer** — RAG prompt design, few-shot examples, and LLM response generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ae541",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Requirements & Setup\n",
    "\n",
    "Install packages (recommended in virtualenv or Colab):\n",
    "\n",
    "```bash\n",
    "pip install -U pip\n",
    "pip install pandas numpy tqdm nltk regex nbformat jupyterlab\n",
    "pip install chromadb langchain sentence-transformers transformers accelerate faiss-cpu\n",
    "pip install openai  # if you plan to use OpenAI embeddings / LLMs\n",
    "pip install cross-encoder  # for reranking (from sentence-transformers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9fc16",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Pipeline Overview (high level)\n",
    "\n",
    "**Embedding Layer**\n",
    "- Parse emails, extract metadata (from, to, subject, date), and body.\n",
    "- Clean (remove quoted text, signatures, long headers).\n",
    "- Chunking strategies to experiment with:\n",
    "  - Fixed-size token windows (e.g., 200–500 tokens) with overlap (e.g., 50 tokens).\n",
    "  - Sentence-based chunking (group by N sentences).\n",
    "  - Thread-based chunking (keep entire thread together).\n",
    "  - Semantically-aware chunking (use paragraph breaks + sentence-transformers clustering).\n",
    "- Embedding model choices:\n",
    "  - OpenAI embeddings (e.g., `text-embedding-3-small` / `text-embedding-3-large`) — higher quality, API needed.\n",
    "  - Local models: SentenceTransformers (`all-MiniLM-L6-v2`, `paraphrase-MPNet-...`) from Hugging Face.\n",
    "\n",
    "**Search Layer**\n",
    "- Index chunks in ChromaDB (or FAISS).\n",
    "- Design 3 test queries that reflect real tasks (see examples below).\n",
    "- Implement caching of query embeddings and top-k results (simple file or Redis-based cache).\n",
    "- Re-ranking via a cross-encoder model (e.g., `cross-encoder/ms-marco-MiniLM-L-6-v2`).\n",
    "\n",
    "**Generation Layer**\n",
    "- Compose a RAG prompt that provides: task instruction, retrieved context (top chunks), citation markers, and few-shot examples.\n",
    "- Use LLM (OpenAI Chat or local Llama-like LLM) to produce final answer.\n",
    "- Include an explanation of which chunks were used (traceability).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187619c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import chromadb\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab25c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_df(df, text_column=\"text\", collection_name=\"emails\"):\n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "    collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "    # Clear existing data\n",
    "    existing = collection.get()\n",
    "    if \"ids\" in existing and existing[\"ids\"]:\n",
    "        collection.delete(ids=existing[\"ids\"])\n",
    "        print(f\"Cleared {len(existing['ids'])} old items.\")\n",
    "\n",
    "    docs = df[text_column].tolist()\n",
    "    embeddings = embedder.encode(docs).tolist()\n",
    "    ids = [f\"email_{i}\" for i in range(len(docs))]\n",
    "\n",
    "    collection.add(documents=docs, embeddings=embeddings, ids=ids)\n",
    "    print(f\"Ingested {len(docs)} records into collection '{collection_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab31baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dataset at: D:/GenAI/Email_Search_AI/enron_emails.csv\n",
      "Cleared 1000 old items.\n",
      "Ingested 1000 records into collection 'emails'.\n",
      "Loaded 1000 emails from CSV.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"D:/GenAI/Email_Search_AI/enron_emails.csv\"\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Found dataset at: {dataset_path}\")\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    text_col = next((c for c in df.columns if c.lower() in [\"text\", \"body\", \"content\", \"message\"]), None)\n",
    "    if text_col:\n",
    "        ingest_df(df, text_column=text_col, collection_name=\"emails\")\n",
    "    else:\n",
    "        print(\" No valid text column found.\")\n",
    "else:\n",
    "    print(\"No dataset found. Using manual input mode.\")\n",
    "\n",
    "if 'email' in df.columns:\n",
    "    sample_texts = df['email'].dropna().tolist()\n",
    "elif 'body' in df.columns:\n",
    "    sample_texts = df['body'].dropna().tolist()\n",
    "else:\n",
    "    raise ValueError(\"CSV must contain a column named 'email' or 'body'\")\n",
    "\n",
    "print(f\"Loaded {len(sample_texts)} emails from CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3611cb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared 1000 old items.\n",
      "Ingested 1000 records into collection 'emails'.\n",
      "Cleared 1000 existing items.\n",
      "Stored 1000 sample emails in vector DB.\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "collection = client.get_or_create_collection(\"emails\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"text\": sample_texts})\n",
    "ingest_df(df, text_column=\"text\", collection_name=\"emails\")\n",
    "\n",
    "# Clear existing collection safely\n",
    "try:\n",
    "    all_data = collection.get()\n",
    "    if 'ids' in all_data and all_data['ids']:\n",
    "        collection.delete(ids=all_data['ids'])\n",
    "        print(f\"Cleared {len(all_data['ids'])} existing items.\")\n",
    "    else:\n",
    "        print(\"Collection is already empty.\")\n",
    "except Exception as e:\n",
    "    print(\"Could not clear collection:\", e)\n",
    "\n",
    "# Embed & add sample_texts\n",
    "embeddings = embedder.encode(sample_texts).tolist()\n",
    "collection.add(documents=sample_texts, embeddings=embeddings, ids=[f\"email_{i}\" for i in range(len(sample_texts))])\n",
    "print(f\"Stored {len(sample_texts)} sample emails in vector DB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d8ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query set to: kubernetes deployment issues\n",
      "\n",
      "Top 3 Results from the Search Layer:\n",
      "1. (-11.322) The project launch has been postponed due to client feedback. We are reviewing the vendor performance metrics for this quarter. The project launch has been postponed due to client feedback. Ensure all documents are updated before the deadline. Security policy updates will be rolled out next week.\n",
      "---\n",
      "2. (-11.343) Please find the attached report for this week’s progress. The project launch has been postponed due to client feedback. The board has approved the new budget for Q2. The project launch has been postponed due to client feedback. Security policy updates will be rolled out next week.\n",
      "---\n",
      "3. (-11.358) The project launch has been postponed due to client feedback. Let’s schedule a follow-up meeting to discuss next steps. We are reviewing the vendor performance metrics for this quarter. Please find the attached report for this week’s progress. Security policy updates will be rolled out next week.\n",
      "---\n",
      "\n",
      "Final Generated Answer:\n",
      "\n",
      "The provided content does not contain any information regarding Kubernetes deployment issues. It primarily discusses project launch postponements, vendor performance metrics, document updates, and security policy updates.\n",
      "\n",
      " Done — Email Search AI workflow completed.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Querying ------------------\n",
    "\n",
    "# Reranker model to use for re-ranking (define here to avoid NameError)\n",
    "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "query = \"kubernetes deployment issues\"\n",
    "print(f\"\\nQuery set to: {query}\")\n",
    "\n",
    "qvec = embedder.encode([query])[0]\n",
    "results = collection.query(query_embeddings=[qvec.tolist()], n_results=3)\n",
    "retrieved_docs = results.get('documents',[[]])[0]\n",
    "\n",
    "\n",
    "if len(retrieved_docs) > 0:\n",
    "    pairs = [(query, doc) for doc in retrieved_docs]\n",
    "    try:\n",
    "        reranker = CrossEncoder(RERANK_MODEL)\n",
    "        scores = reranker.predict(pairs)\n",
    "    except Exception as e:\n",
    "        # If the reranker model cannot be loaded, skip reranking and assign a neutral score\n",
    "        print(\"Reranker unavailable, skipping reranking:\", e)\n",
    "        scores = [0.0] * len(pairs)\n",
    "\n",
    "    ranked = sorted(zip(retrieved_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nTop 3 Results from the Search Layer:\")\n",
    "    for i, (doc, sc) in enumerate(ranked[:3], 1):\n",
    "        print(f\"{i}. ({sc:.3f}) {doc[:500]}\\n---\")\n",
    "else:\n",
    "    print(\"No documents retrieved.\")\n",
    "\n",
    "# Generation: optional (OpenAI)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key='API_KEY_HERE')  # Replace with your actual API key or use env var\n",
    "    context = \"\\n\\n\".join([doc for doc, _ in ranked[:3]]) if 'ranked' in locals() else \"\"\n",
    "    prompt = f\"You are an assistant that answers queries using only the provided email content.\\n\\nQuery: {query}\\n\\nContext:\\n{context}\\n\\nAnswer based only on the context.\" \n",
    "    response = client.chat.completions.create(model='gpt-4o-mini', messages=[{'role':'user','content':prompt}])\n",
    "    print(\"\\nFinal Generated Answer:\\n\")\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(\"\\nSkipping LLM generation — OpenAI API not configured or unavailable.\")\n",
    "    print(e)\n",
    "\n",
    "print('\\n Done — Email Search AI workflow completed.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
